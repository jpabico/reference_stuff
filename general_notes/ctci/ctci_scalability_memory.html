<!DOCTYPE html>
<html>
<head>
    <title>Notes on Scalability and Memory Limits</title>
</head>
<body>
    <h1>System Design Problems</h1>
    <p>Step-by-step approach:
        <ol> 
            <li><strong>Make Believe</strong> - If all the data could fit on 1 machine and there were no memory limitations, how could the problem be solved?</li>
            <li><strong>Get Real</strong> - How much data can one actually fit on a machine? What problems arise when data is split up?  Determine how to divide the data and how a machine would identify where to look up a different piece of data.</li>
            <li><strong>Propose a Solution</strong> - Just like it sounds...</li>
        </ol>
    </p>
    <p>
        Most companies use large systems of interconnected machines.  It's helpful to have a ballpark figure of how much data a computer can store (hard drive space, RAM, internet transfer latency, etc).  
    </p>
    <p>
        Common strategies for dividing up data include:
        <ul> 
            <li><strong>By Order of Appearance</strong> - As new data comes in, current machines fill up before adding additional machines if needed.  Potential problems can include a very large and complex lookup table.</li>
            <li><strong>By Hash Value</strong> - Pick some sort of key relating to the data, hash the key, mod the hash value by the number of machines, and store the data on the machine with that value.  No lookup table is needed since each machine will automatically know where a piece of data is.  However, if a machine exceeds its capacity to store data, it may be required to shift data around to other machines for better load balancing (expensive) or split the machine's data into 2 machines (causing a tree-like structure of machines).</li>
            <li><strong>By Actual Value</strong> - For example, in a social network, someone living in Canada will probably have a lot more friends in Canada than the average Indonesian citizen.  Similar/related data can be grouped together to reduce system latency.</li>
             <li><strong>Arbitrarily</strong> - Implement a (potentially large and complex) lookup table and concentrate on load balancing instead.</li>
        </ul>
    </p>
    <p>
        Example: Given a list of a million documents, how would you find all the documents that contain a list of words?  Assume this procedure will be called repeatedly for the same set of documents (we can therefore accept the burden of pre-processing).
        <ul>
            <li>Try approaching the problem with a smaller sample size.  Pre-process each document and create a hash table index.  Map a word to a list of the documents that contain that word. </li>
            <li>"books" -> {doc2, doc3, doc6, doc8}</li>
            <li>"many" => {doc1, doc3, doc7, doc8, doc9}</li>
            <li>Seach for "many books" by finding the intersection of the two sets of data -> {doc3, doc8}</li>
            <li>Gonig back to the actual large sample size, decide how to divide the data</li>
        </ul>
    </p>
</body>
</html>