<!DOCTYPE html>
<html>
<head>
    <title>Notes on Scalability and Memory Limits</title>
</head>
<body>
    <h1>System Design Problems</h1>
    <p>Step-by-step approach:
        <ol> 
            <li><strong>Make Believe</strong> - If all the data could fit on 1 machine and there were no memory limitations, how could the problem be solved?</li>
            <li><strong>Get Real</strong> - How much data can one actually fit on a machine? What problems arise when data is split up?  Determine how to divide the data and how a machine would identify where to look up a different piece of data.</li>
            <li><strong>Propose a Solution</strong> - Just like it sounds...</li>
        </ol>
    </p>
    <p>
        Most companies use large systems of interconnected machines.  It's helpful to have a ballpark figure of how much data a computer can store (hard drive space, RAM, internet transfer latency, etc).  
    </p>
    <p>
        Common strategies for dividing up data include:
        <ul> 
            <li><strong>By Order of Appearance</strong> - As new data comes in, current machines fill up before adding additional machines if needed.  Potential problems can include a very large and complex lookup table.</li>
            <li><strong>By Hash Value</strong> - Pick some sort of key relating to the data, hash the key, mod the hash value by the number of machines, and store the data on the machine with that value.  No lookup table is needed since each machine will automatically know where a piece of data is.  However, if a machine exceeds its capacity to store data, it may be required to shift data around to other machines for better load balancing (expensive) or split the machine's data into 2 machines (causing a tree-like structure of machines).</li>
            <li><strong>By Actual Value</strong> - For example, in a social network, someone living in Canada will probably have a lot more friends in Canada than the average Indonesian citizen.  Similar/related data can be grouped together to reduce system latency.</li>
             <li><strong>Arbitrarily</strong> - Implement a (potentially large and complex) lookup table and concentrate on load balancing instead.</li>
        </ul>
    </p>
    <p>
        Example: Given a list of a million documents, how would you find all the documents that contain a list of words?  Assume this procedure will be called repeatedly for the same set of documents (we can therefore accept the burden of pre-processing).
        <ul>
            <li>Try approaching the problem with a smaller sample size.  Pre-process each document and create a hash table index.  Map a word to a list of the documents that contain that word. </li>
            <li>"books" -> {doc2, doc3, doc6, doc8}</li>
            <li>"many" => {doc1, doc3, doc7, doc8, doc9}</li>
            <li>Seach for "many books" by finding the intersection of the two sets of data -> {doc3, doc8}</li>
            <li>Going back to the actual large sample size, decide how to divide the data (a given machine...containing the full document list for a given keyword? ...containing the keyword mapping for only a subset of the documents?)</li>
            <li>Decide how to process the data before pushing the results off to another machine. What is this going to look like?</li>
            <li>Set up a lookup table so we know which machine holds a piece of data.</li>
        </ul>
    </p>
    <p>
        Possible solution to the above scenario:
            <ul>
                <li>Divide words alphabetically by keyword so that each machine controls a range of words.</li>
                <li>Implement a simple algorith to iterate through keywords, storing as much data as possible on each machine.  This gives a non-complex and relatively small lookup table.  Each machine can then store a copy of the lookup table.</li>
                <li>A big disadvantage: if new documents/words are added, an expensive shift of keywords may need to be performed.</li>
                <li>To find all the documents that match a list of strings, sort the list alphabetically (preprocessing) and send the part of the list to the appropriate machines.</li>
                <li>Each machine would look up the words and put together a document list to send back to the initial machine.</li>
                <li>The intial machine would determine the intersection of the response results from each machine.</li>
                <li>This is just one possibility...</li>
            </ul>
    </p>
</body>
</html>